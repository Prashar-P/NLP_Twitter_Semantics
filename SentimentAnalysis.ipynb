{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "You may import more packages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from sklearn import svm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import linear_model\n",
    "import os \n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torchtext.data import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.optim import Adam\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt','twitter-test2.txt', 'twitter-test3.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds,testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "\n",
    "    emoji = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    non_alpha_numeric = re.compile('[^a-zA-Z0-9 -]')\n",
    "    url = re.compile('https?:[^\\s]+')\n",
    "    punctuation = re.compile('[^\\w\\s]')\n",
    "    username = re.compile('@[\\w]+')\n",
    "    tags = re.compile('#[\\w]+')\n",
    "    numbers = re.compile(r'\\b\\d+\\b')\n",
    "    single_char = re.compile(r'\\b\\w\\b')\n",
    "\n",
    "    for twt in tweets:\n",
    "        twt[0] = username.sub(r'', twt[0])\n",
    "        twt[0] = tags.sub(r'', twt[0])\n",
    "        twt[0] = url.sub(r'', twt[0])\n",
    "        twt[0] = emoji.sub(r'', twt[0])\n",
    "        twt[0] = punctuation.sub(r'', twt[0])\n",
    "        twt[0] = non_alpha_numeric.sub(r'', twt[0])\n",
    "        twt[0] = numbers.sub(r'', twt[0])\n",
    "        twt[0] = single_char.sub(r'', twt[0])\n",
    "\n",
    "        remove_stop_words = ' '.join(word for word in twt[0].split() if word not in stopwords.words('english'))\n",
    "        twt[0] = remove_stop_words\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # encoded_labels = label_encoder.fit_transform(labels)\n",
    "    label_map = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n",
    "    encoded_labels = [label_map[label] for label in labels]\n",
    "\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "X_testsets = []\n",
    "y_testsets = []\n",
    "test_tweet_id = []\n",
    "\n",
    "for dataset in ['twitter-dev-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    # write code to read in the datasets here\n",
    "    with open(dataset, 'r', encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = str.lower(line)\n",
    "            tweetids[dataset].append(re.findall('^[^\\d]*(\\d+)',line))\n",
    "            tweetgts[dataset].append(re.search(r'(neutral)|(positive)|(negative)',line).group())\n",
    "            tweets[dataset].append(re.findall(r'(?:[neutral|positive|negative])\\b\\s*(.+)',line))\n",
    "            data[dataset].append(line)\n",
    "\n",
    "    # Preprocessing\n",
    "    # Format data for tokenizing - removal of redundant data\n",
    "    tweets[dataset]=(preprocess_tweets(tweets[dataset]))\n",
    "\n",
    "    # Convert labels to numerical values for easier processing\n",
    "    tweetgts[dataset] = encode_labels(tweetgts[dataset])\n",
    "\n",
    "    if dataset in testsets:\n",
    "        X_testsets.append(tweets[dataset])\n",
    "        y_testsets.append(tweetgts[dataset])\n",
    "        test_tweet_id.append(tweetids[dataset])\n",
    "\n",
    "X_train = tweets['twitter-dev-data.txt']\n",
    "y_train = tweetgts['twitter-dev-data.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hey im gonna dublin february know im saying'], ['literally excited im going sam smith concert october'], ['option buy 2gb ram model moto 3rd gen instead 1gb model'], ['little ms philippines'], ['know tpp expanded wars drone strikes mass surveillance'], ['using moto 2nd gen month absolute delight stock android good design best'], ['juan heard green days time life 1st time since leaving florida burst tears miss everyone kellogg'], ['fidel castro died dont worry george soros willing fill shoes wicked man world'], ['cried every episode dream high starting episode tt tomorrow shall watch last final episode'], ['show updates ipad apple tv products today']]\n",
      "[2, 0, 2, 2, 1, 0, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Functions\n",
    "The following section outlines some functions to tokenize and extract features that are used as part of the next classification section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets_with_bert(tweets):\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    encoding = [tokenizer.encode(twt, add_special_tokens=True) for twt in tweets]\n",
    "\n",
    "    encoding = torch.tensor(encoding)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoding)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tweets):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    for text in tweets[0]:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "def bag_of_words(tweets):\n",
    "    tweets = [' '.join(sublist) for sublist in tweets if sublist]\n",
    "    vocab = build_vocab_from_iterator(build_vocab([tweets]), specials=[\"<UNK>\"])\n",
    "    vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    vectorizer = CountVectorizer(max_features=5000,tokenizer=tokenizer)\n",
    "    # print(vocab.get_stoi())\n",
    "    return vectorizer.fit_transform(tweets).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Build an embedding matrix that will be loaded into an Embedding layer later. It must be a matrix\n",
    "#of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-\n",
    "#dimensional vector for the word of index i in our reference word index (built during\n",
    "#tokenization).\n",
    "\n",
    "def gloVe_word_embeddings(tokenized_tweets):\n",
    "    #Set embedding_dim and max_words\n",
    "\n",
    "    max_words = 5000\n",
    "    embedding_dim = 100\n",
    "\n",
    "    glove_dir = 'glove\\glove.6B.100d.txt'\n",
    "    embed_id = {}\n",
    "    with open(glove_dir, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embed_id[word] = coefs\n",
    "            \n",
    "    m_index = 0\n",
    "\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    for i, twts in enumerate(tokenized_tweets):\n",
    "        for word in twts:\n",
    "            if  i < max_words:\n",
    "                embedding_vector = embed_id.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # print(embedding_matrix.shape)\n",
    "    return embedding_matrix,max_words,embedding_dim,embed_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Classifer for part 3\n",
    "\n",
    "The following section outlines some functions to tokenize and extract features that are used as part of the next classification section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    # -- Disable the gradient --\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss  = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds    = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "\n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingLoop(model, loss_fn, optimizer, train_loader, val_loader, epochs=1):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        # Cycle over the training examples (using minibatches)\n",
    "        # X are the examples, Y are the associated labels\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            # Make the prediction\n",
    "            Y_preds = model(X)\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the Test Set - Compute the statistics for the Confusion Matrix\n",
    "def MakePredictions(model, loader):\n",
    "    Y_shuffled, Y_preds = [], []\n",
    "    for X, Y in loader:\n",
    "        preds = model(X)\n",
    "        Y_preds.append(preds)\n",
    "        Y_shuffled.append(Y)\n",
    "    gc.collect()\n",
    "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
    "\n",
    "    return Y_shuffled.detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_matrix,max_words,embedding_dim,embedding = gloVe_word_embeddings(X_train)\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self,max_words,embedding_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(max_words, embedding_dim)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            64,\n",
    "                            num_layers = 2,\n",
    "                            bidirectional = True,\n",
    "                            dropout = 0.2,\n",
    "                            batch_first = True\n",
    "                           )\n",
    "        self.fc = nn.Linear(64 * 2,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        out = self.embedding(X_batch) \n",
    "        out =  self.dropout(out)\n",
    "        out, (hidden_state,cell_state) = self.lstm(out)\n",
    "        out = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        out =  self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def create_LSTM_model(X_train,y_train,X_testsets,y_testsets):\n",
    "    # Set the device to perform the computation\n",
    "    id_preds = []\n",
    "    y_preds = {}\n",
    "\n",
    "    #set seed for reporducibility \n",
    "    setup_seed(42)\n",
    "\n",
    "    text_classifier = TextClassifier(max_words,embedding_dim).to(DEVICE)\n",
    "    # X_train = [twt for twts in X_train for twt in twts]\n",
    "    X_train_bow = bag_of_words(X_train)\n",
    "    X_train_tensor = torch.tensor(X_train_bow[:(len(y_train))])\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(dataset=train_dataset,batch_size=30,shuffle=True, num_workers=10)\n",
    "    \n",
    "    epochs = 2\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    #Loss Func\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = Adam(text_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "    ### Training Loop ###\n",
    "    for i in range(len(X_testsets)):\n",
    "        \n",
    "        X_test = X_testsets[i]\n",
    "        y_test = y_testsets[i]\n",
    "    \n",
    "        # X_test = [twt for twts in X_test for twt in twts]\n",
    "\n",
    "        X_test_bow = bag_of_words(X_test)\n",
    "        X_test_tensor = torch.tensor(X_test_bow[:(len(y_test))])\n",
    "        y_test_tensor = torch.tensor(y_test)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(dataset=test_dataset,batch_size=50, num_workers=10)\n",
    "    \n",
    "        print('training')\n",
    "        TrainingLoop(text_classifier.to(DEVICE), loss_fn, optimizer, train_loader, test_loader, epochs)\n",
    "\n",
    "        torch.save(text_classifier.state_dict(), 'text_classifier.pth')\n",
    "\n",
    "        # Y_actual, Y_preds = MakePredictions(text_classifier, test_loader)\n",
    "        # print(Y_preds)\n",
    "        \n",
    "        # for key, value in zip(test_tweet_id[i], Y_preds):\n",
    "        #     print(value)\n",
    "        #     if value == 0:\n",
    "        #         y_preds[key[0]] = 'positive'\n",
    "        #     elif value == 1:\n",
    "        #         y_preds[key[0]] = 'negative'\n",
    "        #     elif value == 2:\n",
    "        #         y_preds[key[0]] = 'neutral'\n",
    "        # id_preds.append(y_preds)\n",
    "        # break\n",
    "    return id_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_LSTM_model(X_train,y_train,X_testsets,y_testsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_preds(model,X_testsets,features):\n",
    "    id_preds = []\n",
    "    y_preds = {}\n",
    "    for i in range(len(X_testsets)):\n",
    "        X_test = X_testsets[i]\n",
    "        if features == 'bow':\n",
    "            X_test_feature = bag_of_words(X_test)\n",
    "        elif features == 'bert':\n",
    "            X_test_feature = tokenize_tweets_with_bert(X_test)\n",
    "            X_test_feature =  X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
    "        Y_preds = model.predict(np.asarray(X_test_feature))\n",
    "        for key, value in zip(test_tweet_id[i], Y_preds):\n",
    "            if value == 0:\n",
    "                y_preds[key[0]] = 'positive'\n",
    "            elif value == 1:\n",
    "                y_preds[key[0]] = 'negative'\n",
    "            elif value == 2:\n",
    "                y_preds[key[0]] = 'neutral'\n",
    "        id_preds.append(y_preds)\n",
    "    return id_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training maxent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets\\twitter-test1.txt (bow-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.416     0.158     0.426     \n",
      "\n",
      "semeval-tweets\\twitter-test2.txt (bow-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.530     0.109     0.361     \n",
      "\n",
      "semeval-tweets\\twitter-test3.txt (bow-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.434     0.153     0.413     \n",
      "\n",
      "Training maxent\n",
      "semeval-tweets\\twitter-test1.txt (bert-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.416     0.158     0.426     \n",
      "\n",
      "semeval-tweets\\twitter-test2.txt (bert-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.530     0.109     0.361     \n",
      "\n",
      "semeval-tweets\\twitter-test3.txt (bert-maxent): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.434     0.153     0.413     \n",
      "\n",
      "Training svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Priya\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets\\twitter-test1.txt (bow-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.416     0.158     0.426     \n",
      "\n",
      "semeval-tweets\\twitter-test2.txt (bow-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.530     0.109     0.361     \n",
      "\n",
      "semeval-tweets\\twitter-test3.txt (bow-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.434     0.153     0.413     \n",
      "\n",
      "Training svm\n",
      "semeval-tweets\\twitter-test1.txt (bert-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.416     0.158     0.426     \n",
      "\n",
      "semeval-tweets\\twitter-test2.txt (bert-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.530     0.109     0.361     \n",
      "\n",
      "semeval-tweets\\twitter-test3.txt (bert-svm): 0.000\n",
      "            positive  negative  neutral\n",
      "positive    0.000     0.000     0.000     \n",
      "negative    0.000     0.000     0.000     \n",
      "neutral     0.434     0.153     0.413     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Buid traditional sentiment classifiers. An example classifier name 'svm' is given\n",
    "# in the code below. You should replace the other two classifier names\n",
    "# with your own choices. For features used for classifier training, \n",
    "# the 'bow' feature is given in the code. But you could also explore the \n",
    "# use of other features.['svm', 'maxent', 'LSTM']\n",
    "for classifier in ['maxent','svm']:\n",
    "    for features in ['bow','bert']:\n",
    "        # Skeleton: Creation and training of the classifiers\n",
    "        if classifier == 'svm':\n",
    "            print('Training ' + classifier)\n",
    "            #Select feature and format training data to match\n",
    "            if features == 'bow':\n",
    "                X_train_svm = np.asarray(bag_of_words(X_train))\n",
    "            elif features == 'bert':\n",
    "                X_train_svm = tokenize_tweets_with_bert(X_train)\n",
    "                X_train_svm = X_train_svm.reshape(X_train_svm.shape[0], -1)\n",
    "            svm_classifier = svm.SVC(kernel='linear')\n",
    "            svm_classifier.fit(X_train_svm, y_train) \n",
    "            id_preds = get_id_preds(svm_classifier,X_testsets,features)\n",
    "        elif classifier == 'maxent':\n",
    "            print('Training ' + classifier)\n",
    "            if features == 'bow':\n",
    "                X_train_maxent = np.asarray(bag_of_words(X_train))\n",
    "            elif features == 'bert':\n",
    "                X_train_maxent = tokenize_tweets_with_bert(X_train)\n",
    "                X_train_maxent = X_train_maxent.reshape(X_train_maxent.shape[0], -1)\n",
    "            maxent = linear_model.LogisticRegression()\n",
    "            maxent.fit(np.asarray(X_train_maxent),y_train)\n",
    "            id_preds = get_id_preds(maxent,X_testsets,features)\n",
    "        elif classifier == 'LSTM':\n",
    "            print('Training ' + classifier)\n",
    "            # write the LSTM classifier here\n",
    "            # a) Build an embedding matrix \n",
    "            # b) Build and train a neural model built on LSTM.\n",
    "            if features == 'bow':\n",
    "                id_preds = create_LSTM_model(X_train,y_train,X_testsets,y_testsets)\n",
    "            if features == 'bert':\n",
    "                pass\n",
    "        else:\n",
    "            print('Unknown classifier name' + classifier)\n",
    "            continue\n",
    "\n",
    "        all_preds = []\n",
    "         #Predition performance of the classifiers\n",
    "        for i,testset in enumerate(testsets):\n",
    "            id_preds_test = id_preds[i]\n",
    "            all_preds.append(id_preds_test)\n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, features + '-' + classifier)\n",
    "            confusion(id_preds,testset, classifier)\n",
    "\n",
    "            # macro_f1 = f1_score(y_testsets_f1, all_preds_f1, average='macro')\n",
    "            # print(\"Macro-averaged F1 score:\", macro_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
